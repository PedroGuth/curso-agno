{
  "cells": [
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "# üìö **Ingest√£o Completa de PDFs Longos - Pipeline RAG Completo**\n",
     "\n",
     "> *Do PDF bruto at√© o banco vetorial pronto para busca - tudo em um notebook!*\n",
     "\n",
     "---\n",
     "\n",
     "## üéØ **O que voc√™ vai aprender?**\n",
     "\n",
     "Este notebook demonstra um **pipeline completo** para ingest√£o de PDFs longos, incluindo:\n",
     "\n",
     "1. ‚úÖ **Extra√ß√£o de texto** de PDFs (m√∫ltiplos arquivos)\n",
     "2. ‚úÖ **Cria√ß√£o de chunks** (sem√¢nticos e fixos)\n",
     "3. ‚úÖ **Gera√ß√£o de embeddings** \n",
     "4. ‚úÖ **Armazenamento** em m√∫ltiplos bancos vetoriais:\n",
     "   - üóÑÔ∏è **Chroma** (local, r√°pido)\n",
     "   - üöÄ **FAISS** (local, escal√°vel)\n",
     "   - üìä **LanceDB** (moderno, eficiente)\n",
     "\n",
     "---\n",
     "\n",
     "## üìã **Pr√©-requisitos**\n",
     "\n",
     "- Python b√°sico\n",
     "- Pasta `pdfs/` com seus PDFs para processar\n",
     "- Paci√™ncia (processamento pode demorar para PDFs grandes)\n",
     "\n",
     "---\n",
     "\n",
     "**üñºÔ∏è Sugest√£o de imagem**: Um pipeline mostrando PDF ‚Üí Chunks ‚Üí Embeddings ‚Üí Vector Store"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## üöÄ **Setup Inicial - Instalando Depend√™ncias**"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# üöÄ SETUP GRATUITO PARA COLAB/LOCAL\n",
     "# Execute esta c√©lula primeiro para configurar o ambiente!\n",
     "\n",
     "# Instalando depend√™ncias (execute apenas se necess√°rio)\n",
     "!pip install langchain>=0.1.0\n",
     "!pip install langchain-community>=0.0.10\n",
     "!pip install langchain-core>=0.1.0\n",
     "!pip install python-dotenv>=1.0.0\n",
     "!pip install pypdf>=3.15.0\n",
     "!pip install huggingface_hub>=0.19.0\n",
     "!pip install sentence-transformers>=2.2.0\n",
     "!pip install chromadb>=0.4.0\n",
     "!pip install faiss-cpu>=1.7.0\n",
     "!pip install lancedb>=0.4.0\n",
     "!pip install pyarrow>=12.0.0\n",
     "!pip install numpy>=1.24.0\n",
     "!pip install pandas>=2.0.0\n",
     "!pip install tiktoken>=0.5.0\n",
     "\n",
     "print(\"‚úÖ Depend√™ncias instaladas com sucesso!\")\n",
     "print(\"üöÄ Pronto para processar PDFs longos!\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## üì¶ **Importa√ß√µes Necess√°rias**"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# üì¶ IMPORTA√á√ïES PARA INGEST√ÉO DE PDFs\n",
     "import os\n",
     "from pathlib import Path\n",
     "from typing import List, Dict, Any, Optional\n",
     "import time\n",
     "\n",
     "# LangChain\n",
     "from langchain_community.document_loaders import PyPDFLoader\n",
     "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
     "from langchain.text_splitter import CharacterTextSplitter\n",
     "from langchain.text_splitter import TokenTextSplitter\n",
     "from langchain.schema import Document\n",
     "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
     "\n",
     "# Vector Stores\n",
     "from langchain_community.vectorstores import Chroma\n",
     "from langchain_community.vectorstores import FAISS\n",
     "import lancedb\n",
     "import pyarrow as pa\n",
     "\n",
     "# Utilit√°rios\n",
     "import numpy as np\n",
     "import pandas as pd\n",
     "\n",
     "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
     "print(\"üìö Pronto para processar PDFs!\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## üìÑ **Etapa 1: Extra√ß√£o de Texto de PDFs**"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### **1.1: Carregando PDFs Individualmente**"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# üìÑ ETAPA 1: EXTRA√á√ÉO DE TEXTO DE PDFs\n",
     "\n",
     "def carregar_pdf(pdf_path: str) -> List[Document]:\n",
     "    \"\"\"\n",
     "    Carrega um PDF e extrai o texto\n",
     "    \n",
     "    Args:\n",
     "        pdf_path: Caminho para o arquivo PDF\n",
     "    \n",
     "    Returns:\n",
     "        Lista de documentos do LangChain\n",
     "    \"\"\"\n",
     "    try:\n",
     "        print(f\"üìñ Carregando PDF: {Path(pdf_path).name}\")\n",
     "        \n",
     "        loader = PyPDFLoader(pdf_path)\n",
     "        documents = loader.load()\n",
     "        \n",
     "        # Adicionar metadados √∫teis\n",
     "        for i, doc in enumerate(documents):\n",
     "            doc.metadata['source_file'] = Path(pdf_path).name\n",
     "            doc.metadata['page_number'] = i + 1\n",
     "            doc.metadata['total_pages'] = len(documents)\n",
     "        \n",
     "        print(f\"‚úÖ Carregado: {len(documents)} p√°ginas\")\n",
     "        print(f\"üìù Total de caracteres: {sum(len(d.page_content) for d in documents):,}\")\n",
     "        \n",
     "        return documents\n",
     "        \n",
     "    except Exception as e:\n",
     "        print(f\"‚ùå Erro ao carregar {pdf_path}: {e}\")\n",
     "        return []\n",
     "\n",
     "# Exemplo de uso\n",
     "pdf_folder = Path(\"pdfs\")\n",
     "\n",
     "if pdf_folder.exists():\n",
     "    pdf_files = list(pdf_folder.glob(\"*.pdf\"))\n",
     "    \n",
     "    if pdf_files:\n",
     "        print(f\"üìö Encontrados {len(pdf_files)} arquivos PDF\")\n",
     "        print(\"\\n\" + \"=\"*50)\n",
     "        \n",
     "        # Carregar primeiro PDF como exemplo\n",
     "        primeiro_pdf = pdf_files[0]\n",
     "        documentos_exemplo = carregar_pdf(str(primeiro_pdf))\n",
     "        \n",
     "        if documentos_exemplo:\n",
     "            print(\"\\nüìÑ Primeira p√°gina de exemplo:\")\n",
     "            print(\"-\" * 50)\n",
     "            print(documentos_exemplo[0].page_content[:500] + \"...\")\n",
     "    else:\n",
     "        print(f\"‚ö†Ô∏è Nenhum PDF encontrado em {pdf_folder}\")\n",
     "        print(\"üí° Crie uma pasta 'pdfs' e coloque seus PDFs l√°!\")\n",
     "else:\n",
     "    print(f\"‚ö†Ô∏è Pasta {pdf_folder} n√£o existe\")\n",
     "    print(\"üí° Crie uma pasta 'pdfs' e coloque seus PDFs l√°!\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### **1.2: Carregando M√∫ltiplos PDFs**"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# üìö CARREGANDO M√öLTIPLOS PDFs\n",
     "\n",
     "def carregar_todos_pdfs(pdf_folder: str = \"pdfs\") -> List[Document]:\n",
     "    \"\"\"\n",
     "    Carrega todos os PDFs de uma pasta\n",
     "    \n",
     "    Args:\n",
     "        pdf_folder: Caminho para a pasta com PDFs\n",
     "    \n",
     "    Returns:\n",
     "        Lista de todos os documentos\n",
     "    \"\"\"\n",
     "    pdf_folder_path = Path(pdf_folder)\n",
     "    \n",
     "    if not pdf_folder_path.exists():\n",
     "        print(f\"‚ùå Pasta {pdf_folder} n√£o encontrada!\")\n",
     "        return []\n",
     "    \n",
     "    pdf_files = list(pdf_folder_path.glob(\"*.pdf\"))\n",
     "    \n",
     "    if not pdf_files:\n",
     "        print(f\"‚ö†Ô∏è Nenhum PDF encontrado em {pdf_folder}\")\n",
     "        return []\n",
     "    \n",
     "    print(f\"üìö Encontrados {len(pdf_files)} arquivos PDF\")\n",
     "    print(\"=\"*60)\n",
     "    \n",
     "    all_documents = []\n",
     "    \n",
     "    for pdf_file in pdf_files:\n",
     "        print(f\"\\nüìñ Processando: {pdf_file.name}\")\n",
     "        \n",
     "        documentos = carregar_pdf(str(pdf_file))\n",
     "        \n",
     "        if documentos:\n",
     "            all_documents.extend(documentos)\n",
     "            print(f\"‚úÖ Adicionado: {len(documentos)} p√°ginas\")\n",
     "    \n",
     "    print(\"\\n\" + \"=\"*60)\n",
     "    print(f\"‚úÖ Total: {len(all_documents)} p√°ginas de {len(pdf_files)} PDFs\")\n",
     "    print(f\"üìù Total de caracteres: {sum(len(d.page_content) for d in all_documents):,}\")\n",
     "    \n",
     "    return all_documents\n",
     "\n",
     "# Carregar todos os PDFs\n",
     "todos_documentos = carregar_todos_pdfs(\"pdfs\")\n",
     "\n",
     "if todos_documentos:\n",
     "    print(\"\\nüéâ PDFs carregados com sucesso!\")\n",
     "    print(f\"üìä Estat√≠sticas:\")\n",
     "    print(f\"   - Total de p√°ginas: {len(todos_documentos)}\")\n",
     "    print(f\"   - M√©dia de caracteres por p√°gina: {sum(len(d.page_content) for d in todos_documentos) // len(todos_documentos)}\")\n",
     "    print(f\"   - Arquivos √∫nicos: {len(set(d.metadata.get('source_file', '') for d in todos_documentos))}\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## ‚úÇÔ∏è **Etapa 2: Cria√ß√£o de Chunks (Sem√¢nticos e Fixos)**"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### **2.1: Chunks Fixos (Character-based)**"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# ‚úÇÔ∏è ETAPA 2.1: CHUNKS FIXOS (CHARACTER-BASED)\n",
     "\n",
     "def criar_chunks_fixos(documents: List[Document], \n",
     "                       chunk_size: int = 1000, \n",
     "                       chunk_overlap: int = 200) -> List[Document]:\n",
     "    \"\"\"\n",
     "    Cria chunks de tamanho fixo (caracteres)\n",
     "    \n",
     "    Args:\n",
     "        documents: Lista de documentos\n",
     "        chunk_size: Tamanho m√°ximo de cada chunk (caracteres)\n",
     "        chunk_overlap: Sobreposi√ß√£o entre chunks\n",
     "    \n",
     "    Returns:\n",
     "        Lista de chunks\n",
     "    \"\"\"\n",
     "    print(f\"üî™ Criando chunks fixos (tamanho: {chunk_size}, overlap: {chunk_overlap})\")\n",
     "    \n",
     "    splitter = CharacterTextSplitter(\n",
     "        separator=\"\\n\\n\",  # Separador preferencial\n",
     "        chunk_size=chunk_size,\n",
     "        chunk_overlap=chunk_overlap,\n",
     "        length_function=len\n",
     "    )\n",
     "    \n",
     "    chunks = splitter.split_documents(documents)\n",
     "    \n",
     "    print(f\"‚úÖ Criados {len(chunks)} chunks\")\n",
     "    print(f\"üìè Tamanho m√©dio: {sum(len(c.page_content) for c in chunks) // len(chunks)} caracteres\")\n",
     "    print(f\"üìä Tamanho m√≠nimo: {min(len(c.page_content) for c in chunks)} caracteres\")\n",
     "    print(f\"üìä Tamanho m√°ximo: {max(len(c.page_content) for c in chunks)} caracteres\")\n",
     "    \n",
     "    return chunks\n",
     "\n",
     "# Criar chunks fixos (se tiver documentos)\n",
     "if todos_documentos:\n",
     "    chunks_fixos = criar_chunks_fixos(\n",
     "        todos_documentos,\n",
     "        chunk_size=1000,\n",
     "        chunk_overlap=200\n",
     "    )\n",
     "    \n",
     "    print(\"\\nüìÑ Exemplo de chunk fixo:\")\n",
     "    print(\"-\" * 50)\n",
     "    print(chunks_fixos[0].page_content[:300] + \"...\")\n",
     "else:\n",
     "    print(\"‚ö†Ô∏è Carregue PDFs primeiro!\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### **2.2: Chunks Sem√¢nticos (Recursive Character-based)**"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# üß† ETAPA 2.2: CHUNKS SEM√ÇNTICOS (RECURSIVE CHARACTER-BASED)\n",
     "\n",
     "def criar_chunks_semanticos(documents: List[Document], \n",
     "                            chunk_size: int = 1000, \n",
     "                            chunk_overlap: int = 200) -> List[Document]:\n",
     "    \"\"\"\n",
     "    Cria chunks sem√¢nticos respeitando a estrutura do texto\n",
     "    \n",
     "    Args:\n",
     "        documents: Lista de documentos\n",
     "        chunk_size: Tamanho m√°ximo de cada chunk\n",
     "        chunk_overlap: Sobreposi√ß√£o entre chunks\n",
     "    \n",
     "    Returns:\n",
     "        Lista de chunks sem√¢nticos\n",
     "    \"\"\"\n",
     "    print(f\"üß† Criando chunks sem√¢nticos (tamanho: {chunk_size}, overlap: {chunk_overlap})\")\n",
     "    \n",
     "    splitter = RecursiveCharacterTextSplitter(\n",
     "        chunk_size=chunk_size,\n",
     "        chunk_overlap=chunk_overlap,\n",
     "        length_function=len,\n",
     "        # Ordem de prioridade para separadores\n",
     "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"]\n",
     "    )\n",
     "    \n",
     "    chunks = splitter.split_documents(documents)\n",
     "    \n",
     "    print(f\"‚úÖ Criados {len(chunks)} chunks sem√¢nticos\")\n",
     "    print(f\"üìè Tamanho m√©dio: {sum(len(c.page_content) for c in chunks) // len(chunks)} caracteres\")\n",
     "    print(f\"üìä Tamanho m√≠nimo: {min(len(c.page_content) for c in chunks)} caracteres\")\n",
     "    print(f\"üìä Tamanho m√°ximo: {max(len(c.page_content) for c in chunks)} caracteres\")\n",
     "    \n",
     "    return chunks\n",
     "\n",
     "# Criar chunks sem√¢nticos (se tiver documentos)\n",
     "if todos_documentos:\n",
     "    chunks_semanticos = criar_chunks_semanticos(\n",
     "        todos_documentos,\n",
     "        chunk_size=1000,\n",
     "        chunk_overlap=200\n",
     "    )\n",
     "    \n",
     "    print(\"\\nüìÑ Exemplo de chunk sem√¢ntico:\")\n",
     "    print(\"-\" * 50)\n",
     "    print(chunks_semanticos[0].page_content[:300] + \"...\")\n",
     "    \n",
     "    print(\"\\nüìä Compara√ß√£o:\")\n",
     "    print(f\"   - Chunks fixos: {len(chunks_fixos) if 'chunks_fixos' in locals() else 0}\")\n",
     "    print(f\"   - Chunks sem√¢nticos: {len(chunks_semanticos)}\")\n",
     "else:\n",
     "    print(\"‚ö†Ô∏è Carregue PDFs primeiro!\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### **2.3: Chunks Baseados em Tokens**"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# üéØ ETAPA 2.3: CHUNKS BASEADOS EM TOKENS\n",
     "\n",
     "def criar_chunks_tokens(documents: List[Document], \n",
     "                        chunk_size: int = 512, \n",
     "                        chunk_overlap: int = 50) -> List[Document]:\n",
     "    \"\"\"\n",
     "    Cria chunks baseados em tokens (√∫til para modelos de linguagem)\n",
     "    \n",
     "    Args:\n",
     "        documents: Lista de documentos\n",
     "        chunk_size: N√∫mero m√°ximo de tokens por chunk\n",
     "        chunk_overlap: Sobreposi√ß√£o de tokens\n",
     "    \n",
     "    Returns:\n",
     "        Lista de chunks baseados em tokens\n",
     "    \"\"\"\n",
     "    print(f\"üéØ Criando chunks baseados em tokens (tamanho: {chunk_size}, overlap: {chunk_overlap})\")\n",
     "    \n",
     "    try:\n",
     "        splitter = TokenTextSplitter(\n",
     "            chunk_size=chunk_size,\n",
     "            chunk_overlap=chunk_overlap,\n",
     "            encoding_name=\"cl100k_base\"  # Encoding do GPT\n",
     "        )\n",
     "        \n",
     "        chunks = splitter.split_documents(documents)\n",
     "        \n",
     "        print(f\"‚úÖ Criados {len(chunks)} chunks baseados em tokens\")\n",
     "        print(f\"üìè Tamanho m√©dio: {sum(len(c.page_content) for c in chunks) // len(chunks)} caracteres\")\n",
     "        \n",
     "        return chunks\n",
     "        \n",
     "    except Exception as e:\n",
     "        print(f\"‚ö†Ô∏è Erro ao criar chunks baseados em tokens: {e}\")\n",
     "        print(\"üí° Usando chunks sem√¢nticos como alternativa\")\n",
     "        return criar_chunks_semanticos(documents, chunk_size=chunk_size*2, chunk_overlap=chunk_overlap*2)\n",
     "\n",
     "# Criar chunks baseados em tokens (se tiver documentos)\n",
     "if todos_documentos:\n",
     "    chunks_tokens = criar_chunks_tokens(\n",
     "        todos_documentos,\n",
     "        chunk_size=512,\n",
     "        chunk_overlap=50\n",
     "    )\n",
     "    \n",
     "    print(\"\\nüìÑ Exemplo de chunk baseado em tokens:\")\n",
     "    print(\"-\" * 50)\n",
     "    print(chunks_tokens[0].page_content[:300] + \"...\")\n",
     "else:\n",
     "    print(\"‚ö†Ô∏è Carregue PDFs primeiro!\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## üß† **Etapa 3: Gera√ß√£o de Embeddings**"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# üß† ETAPA 3: GERA√á√ÉO DE EMBEDDINGS\n",
     "\n",
     "def configurar_embeddings(modelo: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
     "    \"\"\"\n",
     "    Configura o modelo de embeddings\n",
     "    \n",
     "    Args:\n",
     "        modelo: Nome do modelo de embeddings\n",
     "    \n",
     "    Returns:\n",
     "        Objeto de embeddings configurado\n",
     "    \"\"\"\n",
     "    print(f\"üîß Configurando embeddings: {modelo}\")\n",
     "    \n",
     "    try:\n",
     "        embeddings = HuggingFaceEmbeddings(\n",
     "            model_name=modelo,\n",
     "            model_kwargs={'device': 'cpu'},  # Use 'cuda' se tiver GPU\n",
     "            encode_kwargs={'normalize_embeddings': True}\n",
     "        )\n",
     "        \n",
     "        print(f\"‚úÖ Embeddings configurados!\")\n",
     "        print(f\"üìä Dimens√£o dos embeddings: 384 (para all-MiniLM-L6-v2)\")\n",
     "        \n",
     "        return embeddings\n",
     "        \n",
     "    except Exception as e:\n",
     "        print(f\"‚ùå Erro ao configurar embeddings: {e}\")\n",
     "        return None\n",
     "\n",
     "# Configurar embeddings\n",
     "embeddings = configurar_embeddings()\n",
     "\n",
     "# Testar embeddings (se tiver chunks)\n",
     "if 'chunks_semanticos' in locals() and chunks_semanticos and embeddings:\n",
     "    print(\"\\nüß™ Testando gera√ß√£o de embeddings:\")\n",
     "    \n",
     "    # Gerar embedding de um chunk de exemplo\n",
     "    texto_teste = chunks_semanticos[0].page_content[:500]\n",
     "    \n",
     "    inicio = time.time()\n",
     "    embedding_teste = embeddings.embed_query(texto_teste)\n",
     "    tempo = time.time() - inicio\n",
     "    \n",
     "    print(f\"‚úÖ Embedding gerado em {tempo:.3f}s\")\n",
     "    print(f\"üìä Dimens√£o: {len(embedding_teste)}\")\n",
     "    print(f\"üìä Valores de exemplo: {embedding_teste[:5]}\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## üóÑÔ∏è **Etapa 4: Armazenamento em Bancos Vetoriais**"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### **4.1: Armazenamento no Chroma**"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# üóÑÔ∏è ETAPA 4.1: ARMAZENAMENTO NO CHROMA\n",
     "\n",
     "def armazenar_chroma(chunks: List[Document], \n",
     "                    embeddings: HuggingFaceEmbeddings,\n",
     "                    collection_name: str = \"pdfs_chroma\",\n",
     "                    persist_directory: str = \"./chroma_db\") -> Chroma:\n",
     "    \"\"\"\n",
     "    Armazena chunks no Chroma\n",
     "    \n",
     "    Args:\n",
     "        chunks: Lista de chunks\n",
     "        embeddings: Modelo de embeddings\n",
     "        collection_name: Nome da cole√ß√£o\n",
     "        persist_directory: Diret√≥rio para persist√™ncia\n",
     "    \n",
     "    Returns:\n",
     "        Objeto Chroma configurado\n",
     "    \"\"\"\n",
     "    print(f\"üóÑÔ∏è Armazenando {len(chunks)} chunks no Chroma...\")\n",
     "    \n",
     "    inicio = time.time()\n",
     "    \n",
     "    try:\n",
     "        vectorstore = Chroma.from_documents(\n",
     "            documents=chunks,\n",
     "            embedding=embeddings,\n",
     "            collection_name=collection_name,\n",
     "            persist_directory=persist_directory\n",
     "        )\n",
     "        \n",
     "        tempo = time.time() - inicio\n",
     "        \n",
     "        print(f\"‚úÖ Armazenado no Chroma em {tempo:.2f}s\")\n",
     "        print(f\"üìä Cole√ß√£o: {collection_name}\")\n",
     "        print(f\"üíæ Persistido em: {persist_directory}\")\n",
     "        \n",
     "        return vectorstore\n",
     "        \n",
     "    except Exception as e:\n",
     "        print(f\"‚ùå Erro ao armazenar no Chroma: {e}\")\n",
     "        return None\n",
     "\n",
     "# Armazenar no Chroma (se tiver chunks e embeddings)\n",
     "if 'chunks_semanticos' in locals() and chunks_semanticos and embeddings:\n",
     "    print(\"\\n\" + \"=\"*60)\n",
     "    vectorstore_chroma = armazenar_chroma(\n",
     "        chunks_semanticos,\n",
     "        embeddings,\n",
     "        collection_name=\"pdfs_longos_chroma\"\n",
     "    )\n",
     "    \n",
     "    # Testar busca\n",
     "    if vectorstore_chroma:\n",
     "        print(\"\\nüîç Testando busca no Chroma:\")\n",
     "        query_teste = \"introdu√ß√£o\"\n",
     "        resultados = vectorstore_chroma.similarity_search(query_teste, k=3)\n",
     "        print(f\"‚úÖ Encontrados {len(resultados)} resultados para '{query_teste}'\")\n",
     "        print(f\"\\nüìÑ Primeiro resultado:\")\n",
     "        print(\"-\" * 50)\n",
     "        print(resultados[0].page_content[:200] + \"...\")\n",
     "else:\n",
     "    print(\"‚ö†Ô∏è Crie chunks e configure embeddings primeiro!\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### **4.2: Armazenamento no FAISS**"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# üöÄ ETAPA 4.2: ARMAZENAMENTO NO FAISS\n",
     "\n",
     "def armazenar_faiss(chunks: List[Document], \n",
     "                   embeddings: HuggingFaceEmbeddings,\n",
     "                   index_path: str = \"./faiss_index\"):\n",
     "    \"\"\"\n",
     "    Armazena chunks no FAISS\n",
     "    \n",
     "    Args:\n",
     "        chunks: Lista de chunks\n",
     "        embeddings: Modelo de embeddings\n",
     "        index_path: Caminho para salvar o √≠ndice\n",
     "    \n",
     "    Returns:\n",
     "        Objeto FAISS configurado\n",
     "    \"\"\"\n",
     "    print(f\"üöÄ Armazenando {len(chunks)} chunks no FAISS...\")\n",
     "    \n",
     "    inicio = time.time()\n",
     "    \n",
     "    try:\n",
     "        vectorstore = FAISS.from_documents(\n",
     "            documents=chunks,\n",
     "            embedding=embeddings\n",
     "        )\n",
     "        \n",
     "        # Salvar √≠ndice\n",
     "        vectorstore.save_local(index_path)\n",
     "        \n",
     "        tempo = time.time() - inicio\n",
     "        \n",
     "        print(f\"‚úÖ Armazenado no FAISS em {tempo:.2f}s\")\n",
     "        print(f\"üíæ √çndice salvo em: {index_path}\")\n",
     "        \n",
     "        return vectorstore\n",
     "        \n",
     "    except Exception as e:\n",
     "        print(f\"‚ùå Erro ao armazenar no FAISS: {e}\")\n",
     "        return None\n",
     "\n",
     "def carregar_faiss(index_path: str, embeddings: HuggingFaceEmbeddings):\n",
     "    \"\"\"\n",
     "    Carrega um √≠ndice FAISS existente\n",
     "    \"\"\"\n",
     "    try:\n",
     "        vectorstore = FAISS.load_local(\n",
     "            folder_path=index_path,\n",
     "            embeddings=embeddings\n",
     "        )\n",
     "        print(f\"‚úÖ FAISS carregado de {index_path}\")\n",
     "        return vectorstore\n",
     "    except Exception as e:\n",
     "        print(f\"‚ùå Erro ao carregar FAISS: {e}\")\n",
     "        return None\n",
     "\n",
     "# Armazenar no FAISS (se tiver chunks e embeddings)\n",
     "if 'chunks_semanticos' in locals() and chunks_semanticos and embeddings:\n",
     "    print(\"\\n\" + \"=\"*60)\n",
     "    vectorstore_faiss = armazenar_faiss(\n",
     "        chunks_semanticos,\n",
     "        embeddings,\n",
     "        index_path=\"./faiss_index\"\n",
     "    )\n",
     "    \n",
     "    # Testar busca\n",
     "    if vectorstore_faiss:\n",
     "        print(\"\\nüîç Testando busca no FAISS:\")\n",
     "        query_teste = \"conceitos principais\"\n",
     "        resultados = vectorstore_faiss.similarity_search(query_teste, k=3)\n",
     "        print(f\"‚úÖ Encontrados {len(resultados)} resultados para '{query_teste}'\")\n",
     "        print(f\"\\nüìÑ Primeiro resultado:\")\n",
     "        print(\"-\" * 50)\n",
     "        print(resultados[0].page_content[:200] + \"...\")\n",
     "else:\n",
     "    print(\"‚ö†Ô∏è Crie chunks e configure embeddings primeiro!\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### **4.3: Armazenamento no LanceDB**"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# üìä ETAPA 4.3: ARMAZENAMENTO NO LANCEDB\n",
     "\n",
     "def armazenar_lancedb(chunks: List[Document], \n",
     "                     embeddings: HuggingFaceEmbeddings,\n",
     "                     table_name: str = \"pdfs_lancedb\",\n",
     "                     db_path: str = \"./lancedb\"):\n",
     "    \"\"\"\n",
     "    Armazena chunks no LanceDB\n",
     "    \n",
     "    Args:\n",
     "        chunks: Lista de chunks\n",
     "        embeddings: Modelo de embeddings\n",
     "        table_name: Nome da tabela\n",
     "        db_path: Caminho do banco de dados\n",
     "    \n",
     "    Returns:\n",
     "        Objeto LanceDB configurado\n",
     "    \"\"\"\n",
     "    print(f\"üìä Armazenando {len(chunks)} chunks no LanceDB...\")\n",
     "    \n",
     "    inicio = time.time()\n",
     "    \n",
     "    try:\n",
     "        # Conectar ao banco\n",
     "        db = lancedb.connect(db_path)\n",
     "        \n",
     "        # Preparar dados\n",
     "        dados = []\n",
     "        \n",
     "        print(\"üìù Gerando embeddings e preparando dados...\")\n",
     "        \n",
     "        for i, chunk in enumerate(chunks):\n",
     "            if i % 100 == 0:\n",
     "                print(f\"   Processando chunk {i+1}/{len(chunks)}\")\n",
     "            \n",
     "            # Gerar embedding\n",
     "            embedding = embeddings.embed_query(chunk.page_content)\n",
     "            \n",
     "            # Preparar dados\n",
     "            dados.append({\n",
     "                \"vector\": embedding,\n",
     "                \"text\": chunk.page_content,\n",
     "                \"source\": chunk.metadata.get('source_file', 'unknown'),\n",
     "                \"page\": chunk.metadata.get('page_number', 0),\n",
     "                \"chunk_id\": i\n",
     "            })\n",
     "        \n",
     "        # Criar tabela\n",
     "        print(f\"üóÑÔ∏è Criando tabela '{table_name}'...\")\n",
     "        \n",
     "        if table_name in db.table_names():\n",
     "            table = db.open_table(table_name)\n",
     "            table.add(dados)\n",
     "        else:\n",
     "            table = db.create_table(table_name, dados)\n",
     "        \n",
     "        tempo = time.time() - inicio\n",
     "        \n",
     "        print(f\"‚úÖ Armazenado no LanceDB em {tempo:.2f}s\")\n",
     "        print(f\"üìä Tabela: {table_name}\")\n",
     "        print(f\"üíæ Banco salvo em: {db_path}\")\n",
     "        \n",
     "        return db, table\n",
     "        \n",
     "    except Exception as e:\n",
     "        print(f\"‚ùå Erro ao armazenar no LanceDB: {e}\")\n",
     "        import traceback\n",
     "        traceback.print_exc()\n",
     "        return None, None\n",
     "\n",
     "def buscar_lancedb(table, query: str, embeddings: HuggingFaceEmbeddings, k: int = 5):\n",
     "    \"\"\"\n",
     "    Busca no LanceDB\n",
     "    \"\"\"\n",
     "    query_embedding = embeddings.embed_query(query)\n",
     "    \n",
     "    resultados = table.search(query_embedding).limit(k).to_pandas()\n",
     "    \n",
     "    return resultados\n",
     "\n",
     "# Armazenar no LanceDB (se tiver chunks e embeddings)\n",
     "if 'chunks_semanticos' in locals() and chunks_semanticos and embeddings:\n",
     "    print(\"\\n\" + \"=\"*60)\n",
     "    db_lancedb, table_lancedb = armazenar_lancedb(\n",
     "        chunks_semanticos,\n",
     "        embeddings,\n",
     "        table_name=\"pdfs_longos_lancedb\"\n",
     "    )\n",
     "    \n",
     "    # Testar busca\n",
     "    if table_lancedb:\n",
     "        print(\"\\nüîç Testando busca no LanceDB:\")\n",
     "        query_teste = \"resumo\"\n",
     "        resultados = buscar_lancedb(table_lancedb, query_teste, embeddings, k=3)\n",
     "        print(f\"‚úÖ Encontrados {len(resultados)} resultados para '{query_teste}'\")\n",
     "        if len(resultados) > 0:\n",
     "            print(f\"\\nüìÑ Primeiro resultado:\")\n",
     "            print(\"-\" * 50)\n",
     "            print(resultados.iloc[0]['text'][:200] + \"...\")\n",
     "else:\n",
     "    print(\"‚ö†Ô∏è Crie chunks e configure embeddings primeiro!\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## üéØ **Pipeline Completo - Fun√ß√£o √önica**"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# üéØ PIPELINE COMPLETO - FUN√á√ÉO √öNICA\n",
     "\n",
     "def pipeline_completo_ingestao_pdfs(\n",
     "    pdf_folder: str = \"pdfs\",\n",
     "    chunk_size: int = 1000,\n",
     "    chunk_overlap: int = 200,\n",
     "    chunk_type: str = \"semantico\",  # \"fixo\", \"semantico\", \"token\"\n",
     "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
     "    vector_stores: List[str] = [\"chroma\", \"faiss\", \"lancedb\"]\n",
     ") -> Dict[str, Any]:\n",
     "    \"\"\"\n",
     "    Pipeline completo de ingest√£o de PDFs\n",
     "    \n",
     "    Args:\n",
     "        pdf_folder: Pasta com PDFs\n",
     "        chunk_size: Tamanho dos chunks\n",
     "        chunk_overlap: Sobreposi√ß√£o entre chunks\n",
     "        chunk_type: Tipo de chunk (\"fixo\", \"semantico\", \"token\")\n",
     "        embedding_model: Modelo de embeddings\n",
     "        vector_stores: Lista de bancos vetoriais a usar\n",
     "    \n",
     "    Returns:\n",
     "        Dicion√°rio com todos os vector stores criados\n",
     "    \"\"\"\n",
     "    print(\"üöÄ INICIANDO PIPELINE COMPLETO DE INGEST√ÉO\")\n",
     "    print(\"=\"*60)\n",
     "    \n",
     "    resultado = {}\n",
     "    \n",
     "    # Etapa 1: Carregar PDFs\n",
     "    print(\"\\nüìÑ ETAPA 1: Carregando PDFs...\")\n",
     "    documentos = carregar_todos_pdfs(pdf_folder)\n",
     "    \n",
     "    if not documentos:\n",
     "        print(\"‚ùå Nenhum documento encontrado!\")\n",
     "        return resultado\n",
     "    \n",
     "    # Etapa 2: Criar chunks\n",
     "    print(\"\\n‚úÇÔ∏è ETAPA 2: Criando chunks...\")\n",
     "    if chunk_type == \"fixo\":\n",
     "        chunks = criar_chunks_fixos(documentos, chunk_size, chunk_overlap)\n",
     "    elif chunk_type == \"token\":\n",
     "        chunks = criar_chunks_tokens(documentos, chunk_size, chunk_overlap)\n",
     "    else:  # semantico\n",
     "        chunks = criar_chunks_semanticos(documentos, chunk_size, chunk_overlap)\n",
     "    \n",
     "    # Etapa 3: Configurar embeddings\n",
     "    print(\"\\nüß† ETAPA 3: Configurando embeddings...\")\n",
     "    embeddings = configurar_embeddings(embedding_model)\n",
     "    \n",
     "    if not embeddings:\n",
     "        print(\"‚ùå Erro ao configurar embeddings!\")\n",
     "        return resultado\n",
     "    \n",
     "    # Etapa 4: Armazenar em vector stores\n",
     "    print(\"\\nüóÑÔ∏è ETAPA 4: Armazenando em vector stores...\")\n",
     "    \n",
     "    if \"chroma\" in vector_stores:\n",
     "        print(\"\\n   üì¶ Armazenando no Chroma...\")\n",
     "        resultado[\"chroma\"] = armazenar_chroma(chunks, embeddings)\n",
     "    \n",
     "    if \"faiss\" in vector_stores:\n",
     "        print(\"\\n   üì¶ Armazenando no FAISS...\")\n",
     "        resultado[\"faiss\"] = armazenar_faiss(chunks, embeddings)\n",
     "    \n",
     "    if \"lancedb\" in vector_stores:\n",
     "        print(\"\\n   üì¶ Armazenando no LanceDB...\")\n",
     "        db, table = armazenar_lancedb(chunks, embeddings)\n",
     "        resultado[\"lancedb\"] = {\"db\": db, \"table\": table}\n",
     "    \n",
     "    print(\"\\n\" + \"=\"*60)\n",
     "    print(\"‚úÖ PIPELINE COMPLETO FINALIZADO!\")\n",
     "    print(f\"üìä Estat√≠sticas:\")\n",
     "    print(f\"   - PDFs processados: {len(set(d.metadata.get('source_file', '') for d in documentos))}\")\n",
     "    print(f\"   - P√°ginas totais: {len(documentos)}\")\n",
     "    print(f\"   - Chunks criados: {len(chunks)}\")\n",
     "    print(f\"   - Vector stores: {len(resultado)}\")\n",
     "    \n",
     "    return resultado\n",
     "\n",
     "# Executar pipeline completo\n",
     "print(\"üí° Para executar o pipeline completo, descomente a linha abaixo:\")\n",
     "print(\"# resultado_pipeline = pipeline_completo_ingestao_pdfs(\")\n",
     "print(\"#     pdf_folder='pdfs',\")\n",
     "print(\"#     chunk_size=1000,\")\n",
     "print(\"#     chunk_overlap=200,\")\n",
     "print(\"#     chunk_type='semantico',\")\n",
     "print(\"#     vector_stores=['chroma', 'faiss', 'lancedb']\")\n",
     "print(\"# )\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## üìä **Resumo e Pr√≥ximos Passos**"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# üìä RESUMO DO QUE APRENDEMOS\n",
     "\n",
     "print(\"üéì RESUMO DO PIPELINE DE INGEST√ÉO\")\n",
     "print(\"=\"*60)\n",
     "print(\"\")\n",
     "print(\"‚úÖ Extra√ß√£o de texto de PDFs\")\n",
     "print(\"   - PyPDFLoader do LangChain\")\n",
     "print(\"   - Suporte para m√∫ltiplos PDFs\")\n",
     "print(\"   - Metadados preservados\")\n",
     "print(\"\")\n",
     "print(\"‚úÖ Cria√ß√£o de chunks\")\n",
     "print(\"   - Chunks fixos (Character-based)\")\n",
     "print(\"   - Chunks sem√¢nticos (Recursive)\")\n",
     "print(\"   - Chunks baseados em tokens\")\n",
     "print(\"\")\n",
     "print(\"‚úÖ Gera√ß√£o de embeddings\")\n",
     "print(\"   - HuggingFace Embeddings\")\n",
     "print(\"   - Modelos gratuitos\")\n",
     "print(\"\")\n",
     "print(\"‚úÖ Armazenamento em vector stores\")\n",
     "print(\"   - Chroma (local, r√°pido)\")\n",
     "print(\"   - FAISS (escal√°vel)\")\n",
     "print(\"   - LanceDB (moderno)\")\n",
     "print(\"\")\n",
     "print(\"üöÄ PR√ìXIMOS PASSOS:\")\n",
     "print(\"   1. Use o pipeline_completo_ingestao_pdfs()\")\n",
     "print(\"   2. Integre com seu sistema RAG\")\n",
     "print(\"   3. Configure busca sem√¢ntica\")\n",
     "print(\"   4. Deploy em produ√ß√£o\")\n",
     "print(\"\")\n",
     "print(\"üí° Dicas:\")\n",
     "print(\"   - Ajuste chunk_size e chunk_overlap conforme seu caso\")\n",
     "print(\"   - Use chunks sem√¢nticos para melhor qualidade\")\n",
     "print(\"   - Teste diferentes modelos de embeddings\")\n",
     "print(\"   - Escolha o vector store baseado em suas necessidades\")"
    ]
   }
  ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "name": "python",
    "version": "3.11.0"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
